{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTPDg_hqQtEe"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-Data\" data-toc-modified-id=\"Cleaning-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Cleaning Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Null-values\" data-toc-modified-id=\"Null-values-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Null values</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-Null-Values\" data-toc-modified-id=\"Cleaning-Null-Values-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Cleaning Null Values</a></span></li><li><span><a href=\"#Checking-for-Null-Values\" data-toc-modified-id=\"Checking-for-Null-Values-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Checking for Null Values</a></span></li><li><span><a href=\"#Dropping-Null-Values\" data-toc-modified-id=\"Dropping-Null-Values-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Dropping Null Values</a></span></li><li><span><a href=\"#Filling-Null-Values\" data-toc-modified-id=\"Filling-Null-Values-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Filling Null Values</a></span></li><li><span><a href=\"#ðŸ’¡-Check-for-understanding\" data-toc-modified-id=\"ðŸ’¡-Check-for-understanding-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>ðŸ’¡ Check for understanding</a></span></li></ul></li><li><span><a href=\"#Dealing-with-Duplicates\" data-toc-modified-id=\"Dealing-with-Duplicates-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Dealing with Duplicates</a></span><ul class=\"toc-item\"><li><span><a href=\"#Identifying-Duplicates\" data-toc-modified-id=\"Identifying-Duplicates-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Identifying Duplicates</a></span></li><li><span><a href=\"#Removing-Duplicates\" data-toc-modified-id=\"Removing-Duplicates-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Removing Duplicates</a></span></li><li><span><a href=\"#Removing-Duplicates-Based-on-Specific-Columns\" data-toc-modified-id=\"Removing-Duplicates-Based-on-Specific-Columns-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Removing Duplicates Based on Specific Columns</a></span></li><li><span><a href=\"#Resetting-the-Index\" data-toc-modified-id=\"Resetting-the-Index-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Resetting the Index</a></span></li></ul></li><li><span><a href=\"#Formatting-Data\" data-toc-modified-id=\"Formatting-Data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Formatting Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Formatting-Numeric-Values\" data-toc-modified-id=\"Formatting-Numeric-Values-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Formatting Numeric Values</a></span></li><li><span><a href=\"#Formatting-Strings\" data-toc-modified-id=\"Formatting-Strings-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Formatting Strings</a></span></li><li><span><a href=\"#Formatting-Dates\" data-toc-modified-id=\"Formatting-Dates-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Formatting Dates</a></span></li></ul></li><li><span><a href=\"#Cleaning-Column-Names\" data-toc-modified-id=\"Cleaning-Column-Names-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Cleaning Column Names</a></span></li></ul></li><li><span><a href=\"#Using-apply(),-map(),-and-applymap()\" data-toc-modified-id=\"Using-apply(),-map(),-and-applymap()-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Using <code>apply()</code>, <code>map()</code>, and <code>applymap()</code></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Apply()\" data-toc-modified-id=\"Apply()-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span><code>Apply()</code></a></span></li><li><span><a href=\"#Map()\" data-toc-modified-id=\"Map()-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span><code>Map()</code></a></span></li><li><span><a href=\"#applyMap()\" data-toc-modified-id=\"applyMap()-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span><code>applyMap()</code></a></span></li><li><span><a href=\"#More-examples\" data-toc-modified-id=\"More-examples-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>More examples</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparing-Map-and-Apply\" data-toc-modified-id=\"Comparing-Map-and-Apply-2.0.4.1\"><span class=\"toc-item-num\">2.0.4.1&nbsp;&nbsp;</span>Comparing Map and Apply</a></span></li><li><span><a href=\"#Calculating-the-length-of-the-name\" data-toc-modified-id=\"Calculating-the-length-of-the-name-2.0.4.2\"><span class=\"toc-item-num\">2.0.4.2&nbsp;&nbsp;</span>Calculating the length of the name</a></span></li><li><span><a href=\"#Converting-to-float-some-columns-with-applymap()\" data-toc-modified-id=\"Converting-to-float-some-columns-with-applymap()-2.0.4.3\"><span class=\"toc-item-num\">2.0.4.3&nbsp;&nbsp;</span>Converting to float some columns with applymap()</a></span></li><li><span><a href=\"#Modifying-cloumns-names-with-apply()\" data-toc-modified-id=\"Modifying-cloumns-names-with-apply()-2.0.4.4\"><span class=\"toc-item-num\">2.0.4.4&nbsp;&nbsp;</span>Modifying cloumns names with apply()</a></span></li></ul></li><li><span><a href=\"#ðŸ’¡-Check-for-understanding\" data-toc-modified-id=\"ðŸ’¡-Check-for-understanding-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>ðŸ’¡ Check for understanding</a></span></li><li><span><a href=\"#ðŸ’¡-Check-for-understanding\" data-toc-modified-id=\"ðŸ’¡-Check-for-understanding-2.0.6\"><span class=\"toc-item-num\">2.0.6&nbsp;&nbsp;</span>ðŸ’¡ Check for understanding</a></span></li></ul></li></ul></li><li><span><a href=\"#Filtering-Data\" data-toc-modified-id=\"Filtering-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Filtering Data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Creating-a-condition\" data-toc-modified-id=\"Creating-a-condition-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Creating a condition</a></span></li><li><span><a href=\"#Filtering-df\" data-toc-modified-id=\"Filtering-df-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Filtering df</a></span></li><li><span><a href=\"#Using-multiple-conditions\" data-toc-modified-id=\"Using-multiple-conditions-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Using multiple conditions</a></span></li></ul></li></ul></li><li><span><a href=\"#More-Data-Manipulation\" data-toc-modified-id=\"More-Data-Manipulation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>More Data Manipulation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setting-the-index\" data-toc-modified-id=\"Setting-the-index-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Setting the index</a></span></li><li><span><a href=\"#Adding/removing-rows-and/or-columns\" data-toc-modified-id=\"Adding/removing-rows-and/or-columns-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Adding/removing rows and/or columns</a></span></li><li><span><a href=\"#ðŸ’¡-Check-for-understanding\" data-toc-modified-id=\"ðŸ’¡-Check-for-understanding-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>ðŸ’¡ Check for understanding</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY8QxSBIQtEi"
   },
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60bTqA9jQtEk"
   },
   "source": [
    "## Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul34egOMQtEl"
   },
   "source": [
    "Null values (also known as missing values) are common in datasets and can hinder data analysis and modeling. It is essential to handle null values appropriately to ensure accurate and reliable results. Pandas provides various methods to clean and handle null values in datasets.\n",
    "\n",
    "In Python, `None` is a special constant that represents the absence of a value. It is commonly used to indicate that a variable or function has no value or hasn't been assigned any value. For example, if a function does not explicitly return a value, it implicitly returns `None`.\n",
    "\n",
    "On the other hand, `NaN` stands for \"Not a Number\" and is a special value used to represent missing or undefined numerical data. `NaN` is part of the floating-point representation and is commonly used in numeric data structures like Pandas DataFrames and Series to indicate missing or invalid numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5LcTn6rQtEm"
   },
   "source": [
    "\n",
    "\n",
    "### Cleaning Null Values\n",
    "\n",
    "1. Checking for Null Values:\n",
    "   - Use `isnull()` method to check for null values in a DataFrame or Series.\n",
    "   - Use `notnull()` method to check for non-null values in a DataFrame or Series.\n",
    "\n",
    "2. Dropping Null Values:\n",
    "   - Use `dropna()` method to remove rows with null values from a DataFrame.\n",
    "   - Use `dropna(axis=1)` to remove columns with null values.\n",
    "\n",
    "3. Filling Null Values:\n",
    "   - Use `fillna(value)` method to replace null values with a specific value.\n",
    "   - Use `fillna(method='ffill')` to forward-fill null values with the previous non-null value.\n",
    "   - Use `fillna(method='bfill')` to backward-fill null values with the next non-null value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baZ8OuubQtEn"
   },
   "source": [
    "### Checking for Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfmE3sTTQtEn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset from an online source\n",
    "url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/titanic_train.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtFyKdY5QtEp"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NT--7lkrQtEp"
   },
   "outputs": [],
   "source": [
    "# Checking for Null Values\n",
    "df.isnull()  # Returns a DataFrame with True where values are null\n",
    "# isnull is an alias of isna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWz8OJedQtEr"
   },
   "source": [
    "When working with large datasets, using `isna()` or `isnull()` along with `any()` and `sum()` in Pandas becomes essential for quick and efficient data quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJEHelbGQtEs"
   },
   "outputs": [],
   "source": [
    "# Check for null values in each column\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Yw4qohgQtEt"
   },
   "source": [
    "sum() calculates the sum of each row, considering True as 1 and False as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9V5zJmOyQtEu"
   },
   "outputs": [],
   "source": [
    "# Count the number of null values in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnMRQh_UQtEv"
   },
   "source": [
    "If we add the parameter `axis=1` with the `sum()` function, we can calculate the sum of each row (along the columns) of the DataFrame `df`. This results in a Series that contains the count of null values in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxnMY9G3QtEv"
   },
   "outputs": [],
   "source": [
    "df.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYNmYRS_QtEv"
   },
   "source": [
    "### Dropping Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34mKoWs0QtEw"
   },
   "outputs": [],
   "source": [
    "# Dropping rows with Null Values\n",
    "df.dropna()  # Removes rows with null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uqydYihQtEw"
   },
   "source": [
    "However, as we can see below in the DataFrame, the rows with NaN values have not been removed. To execute the change, it is necessary to use the `inplace=True` option: `df.dropna(inplace=True)` or assign it to a variable such as df = df.dropna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3Fk9iPoQtEw"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znhYDZd_QtEx"
   },
   "outputs": [],
   "source": [
    "# Dropping columns with  Null Values\n",
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr9q_dU9QtEx"
   },
   "source": [
    "In the `dropna()` method of Pandas DataFrame, the `subset`, `how`, and `thresh` parameters are used to control the behavior of dropping rows or columns containing NaN (null) values, when we don't want to drop them just because they have *one* null value:\n",
    "\n",
    "- `subset`: It allows you to specify a subset of columns on which to apply the `dropna()` operation. Only the rows containing NaN values in the specified subset of columns will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XF6VCyJFQtEx"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTflgUzPQtEy"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['Cabin']).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq0sJZ8GQtEy"
   },
   "source": [
    "- `how`: It specifies the condition for dropping rows. It can take the values 'any', which means to drop rows containing any NaN values in the `subset`, or 'all', which means to drop rows containing all NaN values in the `subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6T3W08yQtEy"
   },
   "outputs": [],
   "source": [
    "df.dropna(how='all').tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVBjPHDQQtEz"
   },
   "source": [
    "- `thresh`: It sets a minimum threshold for the number of non-null values that a row must have in the `subset` in order to be kept. Rows with fewer non-null values than the specified threshold will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAPLRDM2QtEz"
   },
   "outputs": [],
   "source": [
    "df.dropna(thresh=3).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0N-dAuwQtEz"
   },
   "source": [
    "### Filling Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQnw3UuzQtE0"
   },
   "source": [
    "`fillna()` is a Pandas method used to replace NaN (null) values in a DataFrame or Series with specified values.\n",
    "- You can use `inplace=True` to modify the DataFrame directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMymw3hJQtE0"
   },
   "outputs": [],
   "source": [
    "# Filling Null Values\n",
    "df.fillna(-1).tail()  # Replaces null values with -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXWly2CEQtE0"
   },
   "source": [
    "Careful if we assign a different data type, since Pandas will change the data type of the whole column. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqS92S7eQtE1"
   },
   "outputs": [],
   "source": [
    "df.dtypes # age is a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jAByrPiQtE1"
   },
   "outputs": [],
   "source": [
    "df_na = df.fillna(\"na\")\n",
    "df_na.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkT-E-r3QtE1"
   },
   "outputs": [],
   "source": [
    "df_na.dtypes # age is not a float anymore, it's an object now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJBz9ppkQtE2"
   },
   "source": [
    "To avoid this, we can select manually in which column to apply the `fillna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Uk4bfYOQtE2"
   },
   "outputs": [],
   "source": [
    "df.Cabin.fillna(\"na\").tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBbuGN-qQtE2"
   },
   "source": [
    "We can also use the mean(), median() etc. to fill the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nAYBD2mQtE2"
   },
   "outputs": [],
   "source": [
    "df.tail() # lets see where we have a NaN in Age in the last rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9-5qhk4QtE2"
   },
   "outputs": [],
   "source": [
    "df.Age.fillna(df.Age.mean()).tail() #after filling with the mean, lets see how it would look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2d7rAwLQtE2"
   },
   "source": [
    "- Two common methods for filling NaN values are `ffill`, which forward fills using the last valid value, and `bfill`, which backward fills using the next valid value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhDJIejjQtE3"
   },
   "outputs": [],
   "source": [
    "# Forward-fill null values in the Age column\n",
    "df['Age'].fillna(method='ffill').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dm5ihCb-QtE3"
   },
   "outputs": [],
   "source": [
    "# Backward-fill null values in the Age column\n",
    "df['Age'].fillna(method='bfill').tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6In8-HcQtE3"
   },
   "source": [
    "### ðŸ’¡ Check for understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKzS4iNfQtE3"
   },
   "source": [
    "Consider the following DataFrame containing information about students:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Cathy', None, 'Eva'],\n",
    "    'Age': [25, 30, None, 22, 28],\n",
    "    'Gender': ['Female', None, 'Female', 'Male', None],\n",
    "    'Score': [90, None, 78, None, 85]\n",
    "}\n",
    "\n",
    "df_students = pd.DataFrame(data)\n",
    "```\n",
    "\n",
    "Your task is to perform the following data cleaning tasks:\n",
    "\n",
    "1. Check for null values in the DataFrame using `isna()` or `isnull()`.\n",
    "\n",
    "2. Replace the null values in the 'Age' column with the average age of the students.\n",
    "\n",
    "3. Replace the null values in the 'Gender' column with \"Female\".\n",
    "\n",
    "4. Drop any rows that have null values in the 'Name' column.\n",
    "\n",
    "5. Forward fill (ffill) the null values in the 'Score' column with the previous valid value.\n",
    "\n",
    "6. After performing all the cleaning steps, print the cleaned DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOSbNi5GQtE5"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXQP1DgXQtE5"
   },
   "source": [
    "## Dealing with Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt2tVjt2QtE5"
   },
   "source": [
    "In data analysis, it's common to encounter duplicate values in datasets. Duplicates can distort our analysis and lead to incorrect conclusions. Fortunately, pandas provides efficient methods to handle duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkIb0yJgQtE5"
   },
   "source": [
    "### Identifying Duplicates\n",
    "\n",
    "To identify duplicate rows in a DataFrame, we can use the `duplicated()` method, which returns a boolean Series indicating whether each row is a duplicate or not. We can then use the `sum()` method to count the total number of duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLaGqlLUQtE6"
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AX9Wxf5hQtE6"
   },
   "outputs": [],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Z7XpMOQtE7"
   },
   "source": [
    "To check for duplicates in specific columns, we can use the `duplicated()` method with the `subset` parameter, or just access first to the column and then check with duplicated().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8U2TAOxQtE7"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates in specific columns\n",
    "df.duplicated(subset=['Age']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Tb0b0SmQtE7"
   },
   "outputs": [],
   "source": [
    "df.Age.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byeaFQ84QtE7"
   },
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "To remove duplicates from a DataFrame, we can use the `drop_duplicates()` method. By default, this method keeps the first occurrence of each duplicated row and removes the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uTYj8hSQtE8"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates and update the DataFrame\n",
    "df.drop_duplicates(inplace=True) # we know there are none but this is how we would do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icLzX7YjQtE8"
   },
   "source": [
    "### Removing Duplicates Based on Specific Columns\n",
    "\n",
    "Sometimes, we may want to remove duplicates based on specific columns. We can pass a subset of column names to the `drop_duplicates()` method to achieve this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goJamaHDQtE8"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates based on specific columns\n",
    "df.drop_duplicates(subset=['Sex', 'Age']) #lets look at the number of rows if we do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0N2DrAa-QtE8"
   },
   "source": [
    "By default, `drop_duplicates()` keeps the first occurrence of each duplicated row. If we want to keep the last occurrence instead, we can set the `keep` parameter to `'last'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydYzGzTRQtE8"
   },
   "outputs": [],
   "source": [
    "# Keep the last occurrence of duplicates\n",
    "df.drop_duplicates(keep='last', inplace=True) # we know there are none but this is how we would do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvsSHvvgQtE9"
   },
   "source": [
    "### Resetting the Index\n",
    "\n",
    "When removing duplicates, the DataFrame index may have gaps due to removed rows. To reset the index after removing duplicates, we can use the `reset_index()` method with the `drop=True` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeFIpwkuQtE9"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates and reset the index\n",
    "df_without_duplicates = df.copy()\n",
    "df_without_duplicates = df.drop_duplicates(subset=['Sex', 'Age'])\n",
    "df_without_duplicates.tail() # look at the gaps in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_JerwJXQtE9"
   },
   "outputs": [],
   "source": [
    "df_without_duplicates.reset_index(drop=True, inplace=True)\n",
    "df_without_duplicates.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtsD503BQtE9"
   },
   "source": [
    "## Formatting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZfNL4-3QtE-"
   },
   "source": [
    "### Formatting Numeric Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uirIKhtqQtE-"
   },
   "source": [
    "\n",
    "1. `round()` Method:\n",
    "   - Rounds numeric values to a specified number of decimal places.\n",
    "\n",
    "2. `format()` Method:\n",
    "   - Formats numeric values as strings for better representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SRAQcpGQtE-"
   },
   "outputs": [],
   "source": [
    "num = 123456.78910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_mExpg-QtE-"
   },
   "outputs": [],
   "source": [
    "round(num,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiNGpKUbQtE-"
   },
   "outputs": [],
   "source": [
    "format(num, '.1f') # .1f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgdsSI39QtE_"
   },
   "source": [
    "### Formatting Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2WVWM1jQtE_"
   },
   "source": [
    "Check the data structures lesson for more string operations and formatting. Here we'll just look at some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c89hWyM6QtE_"
   },
   "source": [
    "1. **Using f-strings (formatted string literals):**\n",
    "   - f-strings are introduced in Python 3.6 and provide a concise and readable way to format strings.\n",
    "   - Place variables or expressions inside curly braces `{}` in the string, preceded by the `f` character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFQsPdziQtE_"
   },
   "outputs": [],
   "source": [
    "name = \"John\"\n",
    "age = 30\n",
    "print(f\"My name is {name} and I am {age} years old.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqUx656eQtFA"
   },
   "source": [
    "2. **Using the `format()` method:**\n",
    "   - The `format()` method can be applied to a string and accepts positional or keyword arguments to replace placeholders.\n",
    "   - Placeholders are represented by curly braces `{}` and can be indexed or named."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SDcxHvGQtFA"
   },
   "outputs": [],
   "source": [
    "print(\"My name is {} and I am {} years old.\".format(name, age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVvNk2PCQtFA"
   },
   "source": [
    "3. **Using `%` formatting:**\n",
    "   - `%` formatting is an older method, similar to C-style formatting, but less recommended due to its limitations and lack of flexibility.\n",
    "   - Placeholders are represented by `%` followed by format specifiers, like `%s` for strings and `%d` for integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz41kWZOQtFA"
   },
   "outputs": [],
   "source": [
    "print(\"My name is %s and I am %d years old.\" % (name, age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxJohFwPQtFB"
   },
   "source": [
    "Let's look at some methods for formatting strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89yNqD3YQtFB"
   },
   "outputs": [],
   "source": [
    "string = 'HELLO world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FS5khP3QtFB"
   },
   "outputs": [],
   "source": [
    "string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKg45GAHQtFB"
   },
   "outputs": [],
   "source": [
    "string.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM2Z8OqcQtFB"
   },
   "outputs": [],
   "source": [
    "string.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKCAlodKQtFB"
   },
   "outputs": [],
   "source": [
    "len(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "va-LyTGyQtFB"
   },
   "outputs": [],
   "source": [
    "'   Hello World   '.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpeeIrhCQtFC"
   },
   "outputs": [],
   "source": [
    "'   Hello World   '.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vp339ANCQtFD"
   },
   "outputs": [],
   "source": [
    "'   Hello World   '.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXErUYtTQtFD"
   },
   "outputs": [],
   "source": [
    "string.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NORbVmw1QtFD"
   },
   "outputs": [],
   "source": [
    "string.replace('HELLO','Hola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyYoFgXzQtFD"
   },
   "source": [
    "### Formatting Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0T_PEDZQtFE"
   },
   "source": [
    "We will study this in another Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkTZ0ctbQtFE"
   },
   "source": [
    "## Cleaning Column Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vcw-nQaQQtFE"
   },
   "source": [
    "We can acccess the columns using `df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4dT-BLZQtFE"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoumDUVkQtFE"
   },
   "source": [
    "In order to modify them, we can assign new column names to `df.columns` by doing `df.columns = [list_of_new_column_names]` or we can use the `rename()` method to just modify a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEkv2BofQtFE"
   },
   "outputs": [],
   "source": [
    "# We need the whole list of new column names\n",
    "# just modified SibSP to sib_sp\n",
    "\n",
    "df.columns = ['passenger_id', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSP',\n",
    "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO0ILj-KQtFF"
   },
   "outputs": [],
   "source": [
    "# We just need a dictionary with the columns we want to modify\n",
    "# Getting the name back as it was\n",
    "\n",
    "df.rename(columns= {'passenger_id': 'PassengerId'}, inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqzj00MjQtFF"
   },
   "source": [
    "# Using `apply()`, `map()`, and `applymap()`\n",
    "\n",
    "- `apply()`\n",
    "    - Apply a custom function to a Series.\n",
    "    - Useful for element-wise transformations.\n",
    "    - Example: `df['squared_numbers'] = df['numbers'].apply(lambda x: x ** 2)`\n",
    "\n",
    "- `map()`\n",
    "    - Transform Series elements based on a dictionary.\n",
    "    - Replaces elements with corresponding dictionary values.\n",
    "    - Example: `df['gender_mapped'] = df['gender'].map({'M': 'Male', 'F': 'Female'})`\n",
    "\n",
    "- `applymap()`\n",
    "    - Apply a custom function to every element in a DataFrame.\n",
    "    - Useful for element-wise transformations on entire DataFrames.\n",
    "    - Example: `df = df.applymap(lambda x: x.upper())`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIECx9DnQtFF"
   },
   "source": [
    "###Â `Apply()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcRJSjiLQtFG"
   },
   "outputs": [],
   "source": [
    "# Applying a custom function using apply()\n",
    "def get_yob(age):\n",
    "    return 1912-age #titanic sank in 1912, we will assume is when Age was recorded\n",
    "\n",
    "df['yob'] = df['Age'].apply(get_yob)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CuF1a4_QtFH"
   },
   "outputs": [],
   "source": [
    "# Apply and applymap are great for using lambda! This is the same as above\n",
    "\n",
    "df['yob'] = df['Age'].apply(lambda age: 1912-age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z39GKWMwQtFI"
   },
   "source": [
    "In the example above, we can see that to create a new column in pandas, we can simply assign a new Series or list to a new column name within the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mWmGSuyQtFI"
   },
   "source": [
    "To edit the information in a whole column in pandas, you can simply assign a new list or array of values to the column you want to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lz20IAwQtFJ"
   },
   "outputs": [],
   "source": [
    "df['Fare'] = df['Fare'].apply(lambda num: round(num, 2))\n",
    "df['Fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWckDgusQtFJ"
   },
   "source": [
    "###Â `Map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL6okbKIQtFJ"
   },
   "outputs": [],
   "source": [
    "# Using map() to transform the 'Sex' column\n",
    "gender_mapping = {'male': 0, 'female': 1}\n",
    "df['Gender_mapped'] = df['Sex'].map(gender_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJhyMk1FQtFJ"
   },
   "outputs": [],
   "source": [
    "# Lets do the same with apply and lambda\n",
    "df.Sex.apply(lambda x: 0 if x==\"male\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3mlX1HKQtFJ"
   },
   "source": [
    "###Â `applyMap()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7EfLBq-QtFJ"
   },
   "outputs": [],
   "source": [
    "# Using applymap() to convert all string columns to uppercase\n",
    "df = df.applymap(lambda x: x.upper() if isinstance(x, str) else x)\n",
    "\n",
    "# Displaying the modified DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7yjJBbQQtFK"
   },
   "source": [
    "### More examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmLW2Y4FQtFK"
   },
   "source": [
    "#### Comparing Map and Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yI0JDkGQtFK"
   },
   "source": [
    "We have a column called \"Embarked\" containing three possible values: 'C', 'Q', and 'S'. We want to map these values to 0, 1 and 2. In this case, `apply()` with a lambda function would be complex due to the if-elif-else conditions, but `map()` can handle it more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0oJXD0hQtFK"
   },
   "outputs": [],
   "source": [
    "# Mapping 'Embarked' values to their full names using map()\n",
    "embarked_mapping = {'C': 0, 'Q': 1, 'S': 2}\n",
    "df['Embarked_nr'] = df['Embarked'].map(embarked_mapping)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df[['Name', 'Embarked_nr']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WnDuPoLQtFK"
   },
   "source": [
    "Why is it a float?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu4Hduf0QtFK"
   },
   "outputs": [],
   "source": [
    "df.isna().sum() # Because Embarked has null values and it converted it to float to handle NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DCQFgbXQtFK"
   },
   "outputs": [],
   "source": [
    "# Mapping 'Embarked' values to their full names using apply() and a lambda function\n",
    "df['Embarked'].apply(lambda x: 0 if x == 'C' else (1 if x == 'Q' else 2))\n",
    "\n",
    "#Note that here it doesn't convert it to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTXdjis6QtFK"
   },
   "source": [
    "#### Calculating the length of the name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efltDgD8QtFK"
   },
   "source": [
    "What if we wanted to create a new column with the length of the name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nsCM5wMQtFL"
   },
   "outputs": [],
   "source": [
    "df['Name_length'] = df.Name.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNv87wuGQtFL"
   },
   "source": [
    "#### Converting to float some columns with applymap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhGUn45mQtFL"
   },
   "source": [
    "Lets look just as an example, how to make float all the following columns: \"PassengerId\", \"Survived\", \"Pclass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58P8VdbHQtFL"
   },
   "outputs": [],
   "source": [
    "df[[\"PassengerId\", \"Survived\", \"Pclass\"]].applymap(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWi3980eQtFL"
   },
   "source": [
    "#### Modifying cloumns names with apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACQlq7fGQtFL"
   },
   "outputs": [],
   "source": [
    "# I could also use the apply function by converting df.columns to Series\n",
    "pd.Series(df.columns).apply(lambda col: col.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gNGDqpjQtFL"
   },
   "source": [
    "### ðŸ’¡ Check for understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aeuFTc-QtFL"
   },
   "source": [
    "Make the column Embarked_nr as an integer type.\n",
    "\n",
    "- If you get an error, read the error, and think how you should proceed.\n",
    "- If you decide to fill the null values, use the mode() since its a categorical variable.\n",
    "- If you get another error, look at what mode() is returning in order to fix the error and convert to integer the Embarked_nr column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qbTO_eiQtFL"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XHknv_tQtFL"
   },
   "source": [
    "### ðŸ’¡ Check for understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFjykI_pQtFL"
   },
   "source": [
    "You are given a dataset of students' exam scores here: https://raw.githubusercontent.com/data-bootcamp-v4/data/main/student_performance.csv. Your task is to perform the following operations using pandas:\n",
    "\n",
    "1. Read the CSV file into a DataFrame.\n",
    "2. Create a new column \"total_score\" that calculates the total score for each student by summing their \"math score,\" \"reading score,\" and \"writing score.\"\n",
    "3. Create a new column \"grade\" that assigns a grade to each student based on the following criteria:\n",
    "   - If the total score is >= 90, the grade is \"A.\"\n",
    "   - If the total score is >= 80 and < 90, the grade is \"B.\"\n",
    "   - If the total score is >= 70 and < 80, the grade is \"C.\"\n",
    "   - If the total score is >= 60 and < 70, the grade is \"D.\"\n",
    "   - If the total score is < 60, the grade is \"F.\"\n",
    "4. Convert all student names in the \"gender\" column to uppercase.\n",
    "5. Create a new column \"is_passed\" that indicates whether each student has passed the exam or not. If the total score is >= 60, the student has passed; otherwise, they have failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKpY-M-fQtFL"
   },
   "outputs": [],
   "source": [
    "df_students = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/student_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hoq7dcG7QtFL"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wl1IDFdBQtFL"
   },
   "source": [
    "# Filtering Data\n",
    "\n",
    "One of the primary tasks in dataset analysis is filtering rows.\n",
    "\n",
    "When filtering DataFrames in Pandas, you can use boolean indexing to select specific rows based on certain conditions. Here's a step-by-step explanation:\n",
    "\n",
    "1. Identify the column(s) you want to use as a filter condition. For example, in `housing_df` the column named 'SalePrice'.\n",
    "\n",
    "2. Create a condition using a comparison operator (e.g., `>`, `<`, `==`, etc.) and the column(s) you want to filter. For instance, to filter all rows where the 'SalePrice' is greater than 10000, you would use `condition = housing_df['SalePrice'] > 10000`.\n",
    "\n",
    "3. Use the condition to filter the DataFrame. You can do this by passing the condition inside square brackets to the DataFrame. For example, `filtered_df = housing_df[condition]` will create a new DataFrame `filtered_df` containing only the rows where the 'SalePrice' is greater than 10000.\n",
    "\n",
    "Keep in mind that the condition should evaluate to a boolean Series with the same length as the DataFrame, indicating which rows to include (True) or exclude (False).\n",
    "\n",
    "You can also combine multiple conditions using logical operators like `&` for 'and' and `|` for 'or'. For instance, to filter rows where the 'SalePrice' is greater than 10000 and the 'FullBath' is more than 1, you can use `condition = (housing_df['SalePrice'] > 10000) & (housing_df['FullBath'] > 1)`.\n",
    "\n",
    "Filtering allows you to extract specific subsets of data from your DataFrame, making it easier to analyze and work with the data that meets your criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQSJffkTQtFM"
   },
   "source": [
    "### Creating a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39MxPzRtQtFM"
   },
   "outputs": [],
   "source": [
    "df.Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFwZym2_QtFM"
   },
   "outputs": [],
   "source": [
    "df.Fare.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GPU16mQQtFM"
   },
   "outputs": [],
   "source": [
    "condition = df.Fare > df.Fare.mean()\n",
    "condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcEtP0LUQtFM"
   },
   "source": [
    "### Filtering df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDMzVRzOQtFM"
   },
   "outputs": [],
   "source": [
    "filtered_df = df[condition]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sW8jHhQXQtFM"
   },
   "outputs": [],
   "source": [
    "# or what is the same, inside the brackets I give True/False Series in order to filter\n",
    "df[df.Fare > df.Fare.mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNZvscvgQtFM"
   },
   "source": [
    "### Using multiple conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OBJGqyNQtFM"
   },
   "outputs": [],
   "source": [
    "# We can combine boolean operators with filters to add conditions\n",
    "# boolean operators: and is &, or is |\n",
    "df[(df.Fare > df.Fare.mean()) & (df.Fare <= 50)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BUCuF4XQtFN"
   },
   "outputs": [],
   "source": [
    "# To avoid using many filters we can also use .isin()\n",
    "\n",
    "df[(df['Fare'] > 100) & (df['Pclass'].isin([2,3]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKZZRhgCQtFN"
   },
   "outputs": [],
   "source": [
    "# We can also use between()\n",
    "\n",
    "df[df['Fare'].between(90,100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJXt4HleQtFN"
   },
   "source": [
    "# More Data Manipulation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rqLdi8jQtFN"
   },
   "source": [
    "## Setting the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PG9jxcpQtFN"
   },
   "source": [
    "To set an index in pandas, you can use the `set_index()` method of the DataFrame. This method allows you to specify which column you want to use as the index for the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9uzyag-QtFN"
   },
   "outputs": [],
   "source": [
    "df.set_index('PassengerId',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eloLWnYQtFN"
   },
   "source": [
    "## Adding/removing rows and/or columns\n",
    "\n",
    "To add or remove rows and/or columns from a pandas DataFrame, you can use the following methods:\n",
    "\n",
    "1. Adding rows:\n",
    "   - Use the `append()` method to add rows to the DataFrame.\n",
    "\n",
    "2. Removing rows:\n",
    "   - Use the `drop()` method with the row index or label to remove specific rows.\n",
    "\n",
    "3. Adding columns:\n",
    "   - Using `df[new_column]`, you simply assign a list, Series, or scalar value to the new column name\n",
    "   - Assign a new column to the DataFrame using bracket notation or the `assign()` method.\n",
    "\n",
    "4. Removing columns:\n",
    "   - Use the `drop()` method with the column name and `axis=1` to remove specific columns.\n",
    "   - Alternatively, you can use the `del` keyword to remove a column in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ryy03oAEQtFN"
   },
   "outputs": [],
   "source": [
    "df.drop(1) # This deletes the row with index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wc4Z4JLxQtFN"
   },
   "outputs": [],
   "source": [
    "# This deletes a column\n",
    "\n",
    "df.drop('Name',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wt_b4DrBQtFO"
   },
   "outputs": [],
   "source": [
    "# This creates a new column\n",
    "\n",
    "df[\"Survived_bool\"] = df['Survived'].map({0: False, 1: True})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CKwHJ3tQtFP"
   },
   "source": [
    "## ðŸ’¡ Check for understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpVQaMg6QtFP"
   },
   "source": [
    "Use the `supermarket_sales.csv` file for this task.\n",
    "\n",
    "1. **Load the Data**: Use pandas to load the `supermarket_sales.csv` file into a DataFrame.\n",
    "\n",
    "2. **Null Values**: Check if the DataFrame has any null values. If there are any, count the number of null values in each column.\n",
    "\n",
    "5. **Formatting Data**: Round any floating point numbers in the DataFrame to two decimal places.\n",
    "\n",
    "6. **Cleaning Column Names**: Ensure all column names are in lowercase and replace any spaces in the column names with underscores.\n",
    "\n",
    "7. **Using `apply()`, `map()`, and `applymap()`**: Create a new column called 'total_cost' which is the product of the 'quantity' and 'unit_price' columns (assuming these columns exist in your dataset). Use the `apply()` function for this.\n",
    "\n",
    "8. **Filtering Data**: Filter the DataFrame to only include rows where 'total_cost' is greater than the average 'total_cost'.\n",
    "\n",
    "9. **Setting the Index**: Set the 'invoice_id' column (or any other unique identifier) as the index of the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU31I5GSQtFP"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/supermarket_sales.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edvpbgEMQtFP"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YwO9eazQtFP"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vNdEciwQtFQ"
   },
   "source": [
    "1. Null Values:\n",
    "   - Null values (also known as missing values) can hinder data analysis and modeling.\n",
    "   - Use `isnull()` or `isna()` to check for null values in a DataFrame or Series.\n",
    "   - Use `any()` and `sum()` to efficiently assess data quality.\n",
    "   - Use `dropna()` to remove rows or columns with null values from a DataFrame.\n",
    "   - Parameters like subset, how, and thresh can control the behavior of dropping rows or columns.\n",
    "   - Use `fillna()` to replace null values with specific values, such as `mean()`, `median()`, or forward/backward fill.\n",
    "\n",
    "5. Formatting Data:\n",
    "   - Use `round()` and `format()` to format numeric values.\n",
    "   - Use f-strings, `format()` or % to format strings and use string methods like `lower()`, `upper()`, `title()`, `strip()`, `split()`, and `replace()`.\n",
    "\n",
    "6. Cleaning Column Names:\n",
    "   - Use df.columns to access column names.\n",
    "   - Modify column names using df.columns or `rename()`.\n",
    "\n",
    "7. Using `apply()`, `map()`, and `applymap()`:\n",
    "   - `apply()`: Applies a custom function to a Series.\n",
    "   - `map()`: Transforms Series elements based on a dictionary.\n",
    "   - `applymap()`: Applies a custom function to every element in a DataFrame.\n",
    "\n",
    "8. Filtering Data:\n",
    "   - Filter rows in a DataFrame using boolean indexing.\n",
    "   - Use comparison operators (<, >, ==) to create conditions.\n",
    "   - Combine multiple conditions using logical operators (& for 'and', | for 'or').\n",
    "\n",
    "9. Setting the Index:\n",
    "   - Use `set_index()` to set an index for the DataFrame.\n",
    "\n",
    "10. Adding/Removing Rows and Columns:\n",
    "   - Use `append()` to add rows to the DataFrame.\n",
    "   - Use `drop()` with the row index/label to remove specific rows.\n",
    "   - Use bracket notation, `assign()`, or `drop()` with axis=1 to add/remove columns."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "537.273px",
    "left": "459.996px",
    "top": "290.824px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
